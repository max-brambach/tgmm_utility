{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nuclear segmentation\n",
    "In this notebook, we use TGMM (McDole et al. 2018) to segment our data from the SPIM.\n",
    "To that end, we are using `tgmm_utility.py` to call the segmentation software and to distribute the tasks across the cores of the computer, to accelerate the segementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import datetime\n",
    "import subprocess\n",
    "from multiprocessing import Pool\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import glob\n",
    "import pandas as pd\n",
    "from scipy.spatial import KDTree\n",
    "import tqdm\n",
    "\n",
    "from SegmentationIO import read_tgmm\n",
    "from tgmm_utility import RunTGMM\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters\n",
    "First, we define the parameters of the segmentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'imgFilePattern': None,  # This is the path to the .klb or .tif file of the nucelar channel\n",
    "    'anisotropyZ':1,  # x/y resolution divided by z resolution (e.g. 0.5um/pixel / 1um/pixel = 2), set to 1 for isotropic resolution\n",
    "    'backgroundThreshold': None,  # voxels below this threshold are background, measure for every image individually\n",
    "    'persistanceSegmentationTau':5,  # watershed level, smaller -> more sensitive (oversegmentation), larger -> less sensitive (merged nuclei)\n",
    "    'maxIterEM':100,  # number of iterations for the GMM fit, reduce to speed up the segmentation\n",
    "    'tolLikelihood':1e-6,  # stopping criterion for GMM fit, increase to speed up, decrease to increase precision\n",
    "    'regularizePrecisionMatrixConstants_lambdaMin':0.02,  # minimum eigenvalue of the GMM precision matrix (inverse covariance matrix), smaller values allow for larger nuclei.\n",
    "    'regularizePrecisionMatrixConstants_lambdaMax':0.1,  # maximum eigenvalue of the GMM precision matrix (inverse covariance matrix), larger values allow for smaller nuclei.\n",
    "    'regularizePrecisionMatrixConstants_maxExcentricity':9.0,  # maximum Excentricity of the fittet GMM (longest axis / shortest axis)\n",
    "    'radiusMedianFilter':1,  # radius of a median filter applied pre-watershed for de-noising, increase if image is noisy, however, small objects will be lost\n",
    "    'minTau':2,  # minimum watershed level\n",
    "    'conn3D':74,  # voxel neighbourhood considered for watershed segmentation, allowed are 6, 28 and 74\n",
    "    'minNucleiSize':8,  # minimum volume (in voxels) of a single nucleus, smaller objects will be disregarded\n",
    "    'maxNucleiSize':3000,  # maximum volume (in voxels) of a single nucleus, larger objects will be disregarded\n",
    "    'maxPercentileTrimSV':0.2,  # maximum percentage of voxels in a watershedded region that are identified as nucleus, decrease for sparse regions, increase for dense regeions\n",
    "    'conn3DsvTrim':6,  # I honestly don't know, but also this doesn't seem to change the result\n",
    "    \n",
    "    'betaPercentageOfN_k':0.05,  # for tracking, unused\n",
    "    'nuPercentageOfN_k':1.0,  # for tracking, unused\n",
    "    'alphaPercentage':0.7,  # for tracking, unused\n",
    "    'temporalWindowForLogicalRules':5,  # for tracking, unused\n",
    "    'thrBackgroundDetectorHigh':1.1,  # for tracking, unused\n",
    "    'thrBackgroundDetectorLow':0.2,  # for tracking, unused\n",
    "    'SLD_lengthTMthr':5,  # for tracking, unused\n",
    "    'estimateOpticalFlow':0,  # for tracking, unused\n",
    "    'maxDistPartitionNeigh':80.0,  # for tracking, unused\n",
    "    'deathThrOpticalFlow':-1,  # for tracking, unused\n",
    "    'maxNumKNNsupervoxel':10,  # for tracking, unused\n",
    "    'maxDistKNNsupervoxel':41.0,  # for tracking, unused\n",
    "    'thrSplitScore':-1.0,  # for tracking, unused\n",
    "    'thrCellDivisionPlaneDistance':12.403,  # for tracking, unused\n",
    "    'thrCellDivisionWithTemporalWindow':0.456,  # for tracking, unused\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We store the paramters in a csv file, to have acces to them later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dd = pd.DataFrame.from_dict(params, orient='index')\n",
    "dd.to_csv('params.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Organisation of image files and paths\n",
    "Next, we set up the required paths to run the segmentations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path to the folder conatining the nuclear images to be segmented (as .klb or .tif)\n",
    "klb_path = Path('path/to/imgages')\n",
    "files = list(klb_path.glob('*.tif'))  # replace '*.tif' with '*.klb', depending on the file type.\n",
    "\n",
    "# path to folder where the segmentation outputs will be saved, needs to exist\n",
    "output_directory = Path('path/to/segmentation/output')\n",
    "\n",
    "# path to TGMM executables (bin folder in the TGMM software, containing the .exe files)\n",
    "local_executables = Path(r'path/to/TGMM/Tracking_GMM_project-v0.3.0-win64/bin')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background trhesholds\n",
    "The TGMM software uses thresholding to filter out background noise. A threshold has to be provided for each image individually. We recommend using FIJI for this purpose. Our workflow was:\n",
    "* Generate the csv file 'bg_thresholds.csv' by running the cell below. This will generate a table with the image paths and the measured threshold (initialised to 0).\n",
    "* Open this table in an external text editor.\n",
    "* Open the nucelar images to be segmented in FIJI one by one.\n",
    "* Measure the background levles in a non-fluorescend part of the sample (e.g. the yolk)\n",
    "* Used the 'adjust contrast' tool to test the identified threshold.\n",
    "* Update the threshold in the 'bg_thresholds.csv' file.\n",
    "* Repeat for all images.\n",
    "* Save csv file as 'bg_thresholds_filled_in.csv'.\n",
    "* Continue running the notebook.\n",
    "\n",
    "It is important to set this threshold correctly. Too high values will lead to drastic undersegmentation in dim regions and too low values will lead to false positives in noisy regions. The segmentation will also take significantly longer/shorter for too low/high values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "bg_threshold_file = pd.DataFrame(files, columns=['file'])\n",
    "bg_threshold_file['threshold'] = 0\n",
    "bg_threshold_file.to_csv(str(klb_path/'bg_thresholds.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bg_threshold = pd.read_csv(klb_path/'bg_thresholds_filled_in.csv')\n",
    "threshold_dict = dict(zip(bg_threshold['file'].values, bg_threshold['threshold'].values))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up the pipeline\n",
    "\n",
    "Next, we set up the pipeline for the nuclear segmentation. We give two examples in the following. The first one, `pipeline`, simply runs runs TGMM on individual images, while the second one, `pipleine_multiple_runs`, runs the segmentation multiple times for different `maxPercentileTrimSV` values to properly segment regions of different density. The results of the multiple runs are then merged into one output file. Note that the points of the different runs can be differentiated by the `run` column. Results are stored as csv files in the output directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pipline(file):\n",
    "    segmentation = RunTGMM(  # generate the object that will manage the segmentation\n",
    "        name=str(file.stem),  # name of the image\n",
    "        output_directory=str(output_directory),  # results folder\n",
    "        local_executables=str(local_executables),  # TGMM executables (bin folder in software package)\n",
    "        parameters=params,  # previously specified segmentation parameters\n",
    "    )\n",
    "    segmentation.update_parameters(  # insert image specific paramters\n",
    "        {'imgFilePattern':str(file)[:-4],  # path to the image\n",
    "        'backgroundThreshold':threshold_dict[str(file)]}  # background level of the image\n",
    "    )\n",
    "    segmentation.generate_tgmm_config()  # generate a configuration file to run TGMM\n",
    "    segmentation.run_watershed_segmentation()  # first step: generate watershed segmentation\n",
    "    segmentation.run_tgmm()  # second step: fit GMMs\n",
    "    segmentation.get_segmentation_as_df()  # read in the TGMM output as pandas DataFrame\n",
    "    df.to_csv(str(output_directory / file.stem) + '.csv')  # store as .csv file\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pipline_multiple_runs(file):\n",
    "    segmentation = RunTGMM(\n",
    "        name=str(file.stem),\n",
    "        output_directory=str(output_directory),\n",
    "        local_executables=str(local_executables),\n",
    "        parameters=params,\n",
    "    )\n",
    "    segmentation.update_parameters(\n",
    "        {'imgFilePattern':str(file)[:-4],\n",
    "        'backgroundThreshold':threshold_dict[str(file)]}\n",
    "    )\n",
    "    segmentation.generate_tgmm_config()\n",
    "    segmentation.run_watershed_segmentation()\n",
    "    for mptsv in [0.2, 0.5, 0.7]:  # loop over different maxPercentileTrimSV values\n",
    "        segmentation.update_parameters({'maxPercentileTrimSV': mptsv})  # update the config file\n",
    "        segmentation.generate_tgmm_config()  # write updated config file\n",
    "        segmentation.run_tgmm()  # run GMM segmentaiton\n",
    "    segmentation.get_segmentation_as_df()\n",
    "    df = segmentation.combine_segmentations()  # combine the runs of different segmentations using a distance metric, points under distance=5 (um) are deleted\n",
    "    df.to_csv(str(output_directory / file.stem) + '.csv')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the segmentation on multiple files\n",
    "Finally, we iterate over the files to generate the segmentations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 39/39 [1:04:51<00:00, 99.78s/it]\n"
     ]
    }
   ],
   "source": [
    "for f in tqdm.tqdm(files):\n",
    "    pipline(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
